{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit-Card Fraud Detection\n",
    "\n",
    "1. Download the dataset from here: [Credit Card Fraud Detection Dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?resource=download)\n",
    "    - Data size is greater than 100 MB to after downloading from Kaggle have compressed it using pandas as shown below:\n",
    "        ```python\n",
    "        # saving to disc as gunzip compressed csv\n",
    "        df.to_csv('dfsavename.csv.gz', compression='gzip')\n",
    "        # reading the compressed file\n",
    "        df = pd.read_csv('dfsavename.csv.gz', compression='gzip')\n",
    "        ```\n",
    "\n",
    "2. In this dataset, there are 31 features out of which 28 features are numerical variables. These features are not the original features but rather the output of principal component analysis.\n",
    "<br/>\n",
    "3. The other 3 variables are: the time of the transaction, the amount of the transaction, and the true class of the transaction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported!\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "\n",
    "color = sns.color_palette()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "# Data-Preparation\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "\n",
    "# ML-Algorithms\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 284807 rows and 31 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_data = pd.read_csv(\"./input/creditcard.csv.gz\")\n",
    "print(f\"The dataset has {credit_data.shape[0]} rows and {credit_data.shape[1]} columns\")\n",
    "credit_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "\n",
    "### 2.1 Basic Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA principal components  \n",
    "useful_cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', \n",
    "               'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', \n",
    "               'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', \n",
    "               'V23', 'V24', 'V25', 'V26', 'V27', 'V28']  # directly fetched using credit_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V1     float64\n",
       "V2     float64\n",
       "V3     float64\n",
       "V4     float64\n",
       "V5     float64\n",
       "V6     float64\n",
       "V7     float64\n",
       "V8     float64\n",
       "V9     float64\n",
       "V10    float64\n",
       "V11    float64\n",
       "V12    float64\n",
       "V13    float64\n",
       "V14    float64\n",
       "V15    float64\n",
       "V16    float64\n",
       "V17    float64\n",
       "V18    float64\n",
       "V19    float64\n",
       "V20    float64\n",
       "V21    float64\n",
       "V22    float64\n",
       "V23    float64\n",
       "V24    float64\n",
       "V25    float64\n",
       "V26    float64\n",
       "V27    float64\n",
       "V28    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_data[useful_cols].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_data.isna().sum().sum()  # No Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>9.481386e+04</td>\n",
       "      <td>47488.145955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54201.500000</td>\n",
       "      <td>84692.000000</td>\n",
       "      <td>139320.500000</td>\n",
       "      <td>172792.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.168375e-15</td>\n",
       "      <td>1.958696</td>\n",
       "      <td>-56.407510</td>\n",
       "      <td>-0.920373</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>1.315642</td>\n",
       "      <td>2.454930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>1.651309</td>\n",
       "      <td>-72.715728</td>\n",
       "      <td>-0.598550</td>\n",
       "      <td>0.065486</td>\n",
       "      <td>0.803724</td>\n",
       "      <td>22.057729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-1.379537e-15</td>\n",
       "      <td>1.516255</td>\n",
       "      <td>-48.325589</td>\n",
       "      <td>-0.890365</td>\n",
       "      <td>0.179846</td>\n",
       "      <td>1.027196</td>\n",
       "      <td>9.382558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V4</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>2.074095e-15</td>\n",
       "      <td>1.415869</td>\n",
       "      <td>-5.683171</td>\n",
       "      <td>-0.848640</td>\n",
       "      <td>-0.019847</td>\n",
       "      <td>0.743341</td>\n",
       "      <td>16.875344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V5</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.380247</td>\n",
       "      <td>-113.743307</td>\n",
       "      <td>-0.691597</td>\n",
       "      <td>-0.054336</td>\n",
       "      <td>0.611926</td>\n",
       "      <td>34.801666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V6</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.487313e-15</td>\n",
       "      <td>1.332271</td>\n",
       "      <td>-26.160506</td>\n",
       "      <td>-0.768296</td>\n",
       "      <td>-0.274187</td>\n",
       "      <td>0.398565</td>\n",
       "      <td>73.301626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V7</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.237094</td>\n",
       "      <td>-43.557242</td>\n",
       "      <td>-0.554076</td>\n",
       "      <td>0.040103</td>\n",
       "      <td>0.570436</td>\n",
       "      <td>120.589494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V8</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.213481e-16</td>\n",
       "      <td>1.194353</td>\n",
       "      <td>-73.216718</td>\n",
       "      <td>-0.208630</td>\n",
       "      <td>0.022358</td>\n",
       "      <td>0.327346</td>\n",
       "      <td>20.007208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-2.406331e-15</td>\n",
       "      <td>1.098632</td>\n",
       "      <td>-13.434066</td>\n",
       "      <td>-0.643098</td>\n",
       "      <td>-0.051429</td>\n",
       "      <td>0.597139</td>\n",
       "      <td>15.594995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V10</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>2.239053e-15</td>\n",
       "      <td>1.088850</td>\n",
       "      <td>-24.588262</td>\n",
       "      <td>-0.535426</td>\n",
       "      <td>-0.092917</td>\n",
       "      <td>0.453923</td>\n",
       "      <td>23.745136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V11</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.673327e-15</td>\n",
       "      <td>1.020713</td>\n",
       "      <td>-4.797473</td>\n",
       "      <td>-0.762494</td>\n",
       "      <td>-0.032757</td>\n",
       "      <td>0.739593</td>\n",
       "      <td>12.018913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V12</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-1.247012e-15</td>\n",
       "      <td>0.999201</td>\n",
       "      <td>-18.683715</td>\n",
       "      <td>-0.405571</td>\n",
       "      <td>0.140033</td>\n",
       "      <td>0.618238</td>\n",
       "      <td>7.848392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V13</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>8.190001e-16</td>\n",
       "      <td>0.995274</td>\n",
       "      <td>-5.791881</td>\n",
       "      <td>-0.648539</td>\n",
       "      <td>-0.013568</td>\n",
       "      <td>0.662505</td>\n",
       "      <td>7.126883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V14</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.207294e-15</td>\n",
       "      <td>0.958596</td>\n",
       "      <td>-19.214325</td>\n",
       "      <td>-0.425574</td>\n",
       "      <td>0.050601</td>\n",
       "      <td>0.493150</td>\n",
       "      <td>10.526766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V15</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>4.887456e-15</td>\n",
       "      <td>0.915316</td>\n",
       "      <td>-4.498945</td>\n",
       "      <td>-0.582884</td>\n",
       "      <td>0.048072</td>\n",
       "      <td>0.648821</td>\n",
       "      <td>8.877742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V16</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.437716e-15</td>\n",
       "      <td>0.876253</td>\n",
       "      <td>-14.129855</td>\n",
       "      <td>-0.468037</td>\n",
       "      <td>0.066413</td>\n",
       "      <td>0.523296</td>\n",
       "      <td>17.315112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V17</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-3.772171e-16</td>\n",
       "      <td>0.849337</td>\n",
       "      <td>-25.162799</td>\n",
       "      <td>-0.483748</td>\n",
       "      <td>-0.065676</td>\n",
       "      <td>0.399675</td>\n",
       "      <td>9.253526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V18</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>9.564149e-16</td>\n",
       "      <td>0.838176</td>\n",
       "      <td>-9.498746</td>\n",
       "      <td>-0.498850</td>\n",
       "      <td>-0.003636</td>\n",
       "      <td>0.500807</td>\n",
       "      <td>5.041069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V19</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.039917e-15</td>\n",
       "      <td>0.814041</td>\n",
       "      <td>-7.213527</td>\n",
       "      <td>-0.456299</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>0.458949</td>\n",
       "      <td>5.591971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V20</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>6.406204e-16</td>\n",
       "      <td>0.770925</td>\n",
       "      <td>-54.497720</td>\n",
       "      <td>-0.211721</td>\n",
       "      <td>-0.062481</td>\n",
       "      <td>0.133041</td>\n",
       "      <td>39.420904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V21</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.654067e-16</td>\n",
       "      <td>0.734524</td>\n",
       "      <td>-34.830382</td>\n",
       "      <td>-0.228395</td>\n",
       "      <td>-0.029450</td>\n",
       "      <td>0.186377</td>\n",
       "      <td>27.202839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V22</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-3.568593e-16</td>\n",
       "      <td>0.725702</td>\n",
       "      <td>-10.933144</td>\n",
       "      <td>-0.542350</td>\n",
       "      <td>0.006782</td>\n",
       "      <td>0.528554</td>\n",
       "      <td>10.503090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V23</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>0.624460</td>\n",
       "      <td>-44.807735</td>\n",
       "      <td>-0.161846</td>\n",
       "      <td>-0.011193</td>\n",
       "      <td>0.147642</td>\n",
       "      <td>22.528412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V24</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>4.473266e-15</td>\n",
       "      <td>0.605647</td>\n",
       "      <td>-2.836627</td>\n",
       "      <td>-0.354586</td>\n",
       "      <td>0.040976</td>\n",
       "      <td>0.439527</td>\n",
       "      <td>4.584549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V25</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>0.521278</td>\n",
       "      <td>-10.295397</td>\n",
       "      <td>-0.317145</td>\n",
       "      <td>0.016594</td>\n",
       "      <td>0.350716</td>\n",
       "      <td>7.519589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V26</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.683437e-15</td>\n",
       "      <td>0.482227</td>\n",
       "      <td>-2.604551</td>\n",
       "      <td>-0.326984</td>\n",
       "      <td>-0.052139</td>\n",
       "      <td>0.240952</td>\n",
       "      <td>3.517346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V27</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-3.660091e-16</td>\n",
       "      <td>0.403632</td>\n",
       "      <td>-22.565679</td>\n",
       "      <td>-0.070840</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.091045</td>\n",
       "      <td>31.612198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V28</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-1.227390e-16</td>\n",
       "      <td>0.330083</td>\n",
       "      <td>-15.430084</td>\n",
       "      <td>-0.052960</td>\n",
       "      <td>0.011244</td>\n",
       "      <td>0.078280</td>\n",
       "      <td>33.847808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amount</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>8.834962e+01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>25691.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.727486e-03</td>\n",
       "      <td>0.041527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count          mean           std         min           25%  \\\n",
       "Time    284807.0  9.481386e+04  47488.145955    0.000000  54201.500000   \n",
       "V1      284807.0  1.168375e-15      1.958696  -56.407510     -0.920373   \n",
       "V2      284807.0  3.416908e-16      1.651309  -72.715728     -0.598550   \n",
       "V3      284807.0 -1.379537e-15      1.516255  -48.325589     -0.890365   \n",
       "V4      284807.0  2.074095e-15      1.415869   -5.683171     -0.848640   \n",
       "V5      284807.0  9.604066e-16      1.380247 -113.743307     -0.691597   \n",
       "V6      284807.0  1.487313e-15      1.332271  -26.160506     -0.768296   \n",
       "V7      284807.0 -5.556467e-16      1.237094  -43.557242     -0.554076   \n",
       "V8      284807.0  1.213481e-16      1.194353  -73.216718     -0.208630   \n",
       "V9      284807.0 -2.406331e-15      1.098632  -13.434066     -0.643098   \n",
       "V10     284807.0  2.239053e-15      1.088850  -24.588262     -0.535426   \n",
       "V11     284807.0  1.673327e-15      1.020713   -4.797473     -0.762494   \n",
       "V12     284807.0 -1.247012e-15      0.999201  -18.683715     -0.405571   \n",
       "V13     284807.0  8.190001e-16      0.995274   -5.791881     -0.648539   \n",
       "V14     284807.0  1.207294e-15      0.958596  -19.214325     -0.425574   \n",
       "V15     284807.0  4.887456e-15      0.915316   -4.498945     -0.582884   \n",
       "V16     284807.0  1.437716e-15      0.876253  -14.129855     -0.468037   \n",
       "V17     284807.0 -3.772171e-16      0.849337  -25.162799     -0.483748   \n",
       "V18     284807.0  9.564149e-16      0.838176   -9.498746     -0.498850   \n",
       "V19     284807.0  1.039917e-15      0.814041   -7.213527     -0.456299   \n",
       "V20     284807.0  6.406204e-16      0.770925  -54.497720     -0.211721   \n",
       "V21     284807.0  1.654067e-16      0.734524  -34.830382     -0.228395   \n",
       "V22     284807.0 -3.568593e-16      0.725702  -10.933144     -0.542350   \n",
       "V23     284807.0  2.578648e-16      0.624460  -44.807735     -0.161846   \n",
       "V24     284807.0  4.473266e-15      0.605647   -2.836627     -0.354586   \n",
       "V25     284807.0  5.340915e-16      0.521278  -10.295397     -0.317145   \n",
       "V26     284807.0  1.683437e-15      0.482227   -2.604551     -0.326984   \n",
       "V27     284807.0 -3.660091e-16      0.403632  -22.565679     -0.070840   \n",
       "V28     284807.0 -1.227390e-16      0.330083  -15.430084     -0.052960   \n",
       "Amount  284807.0  8.834962e+01    250.120109    0.000000      5.600000   \n",
       "Class   284807.0  1.727486e-03      0.041527    0.000000      0.000000   \n",
       "\n",
       "                 50%            75%            max  \n",
       "Time    84692.000000  139320.500000  172792.000000  \n",
       "V1          0.018109       1.315642       2.454930  \n",
       "V2          0.065486       0.803724      22.057729  \n",
       "V3          0.179846       1.027196       9.382558  \n",
       "V4         -0.019847       0.743341      16.875344  \n",
       "V5         -0.054336       0.611926      34.801666  \n",
       "V6         -0.274187       0.398565      73.301626  \n",
       "V7          0.040103       0.570436     120.589494  \n",
       "V8          0.022358       0.327346      20.007208  \n",
       "V9         -0.051429       0.597139      15.594995  \n",
       "V10        -0.092917       0.453923      23.745136  \n",
       "V11        -0.032757       0.739593      12.018913  \n",
       "V12         0.140033       0.618238       7.848392  \n",
       "V13        -0.013568       0.662505       7.126883  \n",
       "V14         0.050601       0.493150      10.526766  \n",
       "V15         0.048072       0.648821       8.877742  \n",
       "V16         0.066413       0.523296      17.315112  \n",
       "V17        -0.065676       0.399675       9.253526  \n",
       "V18        -0.003636       0.500807       5.041069  \n",
       "V19         0.003735       0.458949       5.591971  \n",
       "V20        -0.062481       0.133041      39.420904  \n",
       "V21        -0.029450       0.186377      27.202839  \n",
       "V22         0.006782       0.528554      10.503090  \n",
       "V23        -0.011193       0.147642      22.528412  \n",
       "V24         0.040976       0.439527       4.584549  \n",
       "V25         0.016594       0.350716       7.519589  \n",
       "V26        -0.052139       0.240952       3.517346  \n",
       "V27         0.001342       0.091045      31.612198  \n",
       "V28         0.011244       0.078280      33.847808  \n",
       "Amount     22.000000      77.165000   25691.160000  \n",
       "Class       0.000000       0.000000       1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArsAAAIWCAYAAABX4UWyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAaaklEQVR4nO3db4xld33f8c+XXRwggcHgxHJtF4wYVVqnEoFqcZQ8IEDN2kq7joIQf4pXxAKixRUppI1DUezipCKpgMYt9gNii3VKcRwCxUFQszUkoApjE0KBtQu7ECzv1tgNu4wdSAHDrw/mbHszrHd38M7c3e+8XtLV3Ps759zzm30wfvvo3N+tMUYAAKCjx8x7AgAAsFbELgAAbYldAADaErsAALQldgEAaGvzvCcwb0tLS5ajAABoYGFhoVaOubILAEBbYhcAgLbELgAAbYldAADaErsAALQldgEAaEvsAgDQltgFAKAtsQsAQFtiFwCAtsQuAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0NbmeU+A5Pxr75j3FIA1tmfn1nlPAWBDcmUXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbELAEBbYhcAgLbELgAAbYldAADaErsAALQldgEAaEvsAgDQltgFAKAtsQsAQFtiFwCAtsQuAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbELAEBbYhcAgLbELgAAbYldAADaErsAALQldgEAaEvsAgDQltgFAKAtsQsAQFtiFwCAtsQuAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0tS6xW1XnVtXHq+quqtpTVa+fxq+qqgNV9bnpcfHMMb9ZVfuq6ktV9aKZ8W3T2L6qumJm/Lyq+vQ0/kdVddo0/mPT633T9qevx+8MAMD8rdeV3YeTvHGMsSXJBUleV1Vbpm3vGGM8a3p8OEmmbS9Ncn6SbUmurapNVbUpyTuTXJRkS5KXzbzP707v9cwkh5JcNo1fluTQNP6OaT8AADaAdYndMcZ9Y4zPTs8fSnJ3krOPcsj2JDeNMb4zxvirJPuSbJ0e+8YYXx1jfDfJTUm2V1UleX6S903H70pyycx77Zqevy/JC6b9AQBobt3v2Z1uI/iZJJ+ehi6vqs9X1Q1Vdfo0dnaSe2cO2z+NPdL4U5N8c4zx8Irxv/Ne0/alaX8AAJrbvJ4nq6qfSPInSX5tjPFgVV2X5OokY/r5tiS/sp5zmrV37955nRpozt8XgLWxuLh41O3rFrtV9dgsh+57xhjvT5Ixxv0z29+V5EPTywNJzp05/JxpLI8w/o0kT66qzdPV29n9D7/X/qranGRh2v+HHOsfa83svmM+5wXWzdz+vgBscOu1GkMluT7J3WOMt8+MnzWz2y8l+eL0/JYkL51WUjgvyWKSO5LcmWRxWnnhtCx/iO2WMcZI8vEkL56O35HkgzPvtWN6/uIkH5v2BwCgufW6svtzSV6Z5AtV9blp7E1ZXk3hWVm+jeFrSV6bJGOMPVV1c5K7srySw+vGGN9Pkqq6PMmtSTYluWGMsWd6v99IclNV/XaSv8xyXGf6+YdVtS/JwSwHMgAAG0Bt9IucS0tLc/8HOP9atzFAd3t2bp33FADaW1hY+KEVt3yDGgAAbYldAADaErsAALQldgEAaEvsAgDQltgFAKAtsQsAQFtiFwCAtsQuAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbELAEBbYhcAgLbELgAAbYldAADaErsAALQldgEAaEvsAgDQltgFAKAtsQsAQFtiFwCAtsQuAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbELAEBbYhcAgLbELgAAbYldAADaErsAALQldgEAaEvsAgDQltgFAKAtsQsAQFtiFwCAtsQuAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbELAEBbYhcAgLbELgAAba1L7FbVuVX18aq6q6r2VNXrp/GnVNXuqto7/Tx9Gq+quqaq9lXV56vq2TPvtWPaf29V7ZgZf05VfWE65pqqqqOdAwCA/tbryu7DSd44xtiS5IIkr6uqLUmuSHLbGGMxyW3T6yS5KMni9HhNkuuS5XBNcmWS5ybZmuTKmXi9LsmrZ47bNo0/0jkAAGhuXWJ3jHHfGOOz0/OHktyd5Owk25PsmnbbleSS6fn2JDeOZbcneXJVnZXkRUl2jzEOjjEOJdmdZNu07UljjNvHGCPJjSve60jnAACguXW/Z7eqnp7kZ5J8OsmZY4z7pk1fT3Lm9PzsJPfOHLZ/Gjva+P4jjOco5wAAoLnN63myqvqJJH+S5NfGGA9Ot9UmScYYo6rGWp7/WOfYu3fvWp4e2MD8fQFYG4uLi0fdvm6xW1WPzXLovmeM8f5p+P6qOmuMcd90K8ID0/iBJOfOHH7ONHYgyfNWjP/ZNH7OEfY/2jl+yLH+sdbM7jvmc15g3czt7wvABrdeqzFUkuuT3D3GePvMpluSHF5RYUeSD86MXzqtynBBkqXpVoRbk1xYVadPH0y7MMmt07YHq+qC6VyXrnivI50DAIDm1uvK7s8leWWSL1TV56axNyV5a5Kbq+qyJPckecm07cNJLk6yL8m3k7wqScYYB6vq6iR3Tvu9ZYxxcHq+M8m7kzw+yUemR45yDgAAmqvlxQs2rqWlpbn/A5x/rdsYoLs9O7fOewoA7S0sLNTKMd+gBgBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbELAEBbYhcAgLbELgAAbYldAADaErsAALQldgEAaEvsAgDQltgFAKAtsQsAQFtiFwCAtsQuAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbELAEBbYhcAgLbELgAAbYldAADaErsAALQldgEAaEvsAgDQltgFAKAtsQsAQFtiFwCAtsQuAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbELAEBbYhcAgLbELgAAbYldAADaErsAALQldgEAaEvsAgDQ1nHHblX9+iOMv+HETQcAAE6c1VzZ/a1HGH/ziZgIAACcaJuPtUNVPX96uqmqfiFJzWx+RpKH1mJiAADwaB0zdpNcP/18XJIbZsZHkq8n+ecnelIAAHAiHDN2xxjnJUlV3TjGuHTtpwQAACfG8VzZTZLMhm5VPWbFth+cyEkBAMCJsJrVGJ5dVZ+qqm8l+d70eHj6CQAAJ53jvrKbZFeSP03yK0m+vTbTAQCAE2c1sfu0JP96jDHWajIAAHAirWad3Q8kuXCtJgIAACfaamL3cUk+UFUfraobZx/HOrCqbqiqB6rqizNjV1XVgar63PS4eGbbb1bVvqr6UlW9aGZ82zS2r6qumBk/r6o+PY3/UVWdNo3/2PR637T96av4fQEAOMWtJnbvSvK7Sf57kq+seBzLu5NsO8L4O8YYz5oeH06SqtqS5KVJzp+OubaqNlXVpiTvTHJRki1JXjbtm2le7xhjPDPJoSSXTeOXJTk0jb9j2g8AgA1iNUuP/Zsf9SRjjE+s4qrq9iQ3jTG+k+Svqmpfkq3Ttn1jjK8mSVXdlGR7Vd2d5PlJXj7tsyvJVUmum97rqmn8fUn+Y1WV+44BADaG447dma8N/iFjjI/9iOe/vKouTfKZJG8cYxxKcnaS22f22T+NJcm9K8afm+SpSb45xnj4CPufffiYMcbDVbU07f/XR5rM3r17f8RfA+Do/H0BWBuLi4tH3b6a1RiuX/H6J5OcluW4fMbqppVk+crr1Vn+2uGrk7wty8uazc2x/rHWzO475nNeYN3M7e8LwAa3mtsYzpt9Pd1D++YkD/0oJx5j3D/zXu9K8qHp5YEk587ses40lkcY/0aSJ1fV5unq7uz+h99rf1VtTrIw7Q8AwAawmg+o/R1jjO8n+Z0k/+pHOb6qzpp5+UtJDq/UcEuSl04rKZyXZDHJHUnuTLI4rbxwWpY/xHbLdP/tx5O8eDp+R5IPzrzXjun5i5N8zP26AAAbx2puYziSf5zkB8faqarem+R5Sc6oqv1JrkzyvKp6VpZvY/haktcmyRhjT1XdnOXVHx5O8roprFNVlye5NcmmJDeMMfZMp/iNJDdV1W8n+cv8/1surk/yh9OH3A5mOZABANgg6ngvdFbVvVkO08OekOW1d3eOMY651u7Jamlpae5Xes+/1j270N2enVuPvRMAj8rCwkKtHFvNld1/tuL1t5J8eYzx4KOaFQAArJHVfEDtz5Okqh6T5Mwk948xjnkLAwAAzMtxf0Ctqp44fTXw32Z5lYO/rapdVbWwZrMDAIBHYTWrMfyHJD+e5B8mefz08wlJrlmDeQEAwKO2mnt2tyV5xhjj29PrL1fVq5J85cRPCwAAHr3VXNn9P1n+1rRZZyT5zombDgAAnDirubL7B0l2V9Xbk9yT5GlJ/kWSd63FxAAA4NFaTez+TpY/mPaKJH8vyf9K8ntjjOuPehQAAMzJam5j+P0kXxpjvHCMsWWM8cIkd1fVv1+bqQEAwKOzmth9WZLPrBj7iyQvP3HTAQCAE2c1sTuSbFoxtmmV7wEAAOtmNaH6ySRXT9+gdvib1K6axgEA4KSzmg+ovT7Jh5LcV1X3JPn7Se5L8k/WYmIAAPBoHXfsjjH2V9Wzk2xNcm6Se5PcMcb4wVpNDgAAHo3VXNnNFLa3Tw8AADip+XAZAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbELAEBbYhcAgLbELgAAbYldAADaErsAALQldgEAaEvsAgDQltgFAKAtsQsAQFtiFwCAtsQuAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbELAEBbYhcAgLbELgAAbYldAADaErsAALQldgEAaEvsAgDQltgFAKAtsQsAQFtiFwCAtsQuAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2lqX2K2qG6rqgar64szYU6pqd1XtnX6ePo1XVV1TVfuq6vNV9eyZY3ZM+++tqh0z48+pqi9Mx1xTVXW0cwAAsDGs15XddyfZtmLsiiS3jTEWk9w2vU6Si5IsTo/XJLkuWQ7XJFcmeW6SrUmunInX65K8eua4bcc4BwAAG8C6xO4Y4xNJDq4Y3p5k1/R8V5JLZsZvHMtuT/LkqjoryYuS7B5jHBxjHEqyO8m2aduTxhi3jzFGkhtXvNeRzgEAwAaweY7nPnOMcd/0/OtJzpyen53k3pn99k9jRxvff4Txo53jiPbu3bvKXwHg+Pj7ArA2FhcXj7p9nrH7/4wxRlWNeZ/jWP9Ya2b3HfM5L7Bu5vb3BWCDm+dqDPdPtyBk+vnANH4gybkz+50zjR1t/JwjjB/tHAAAbADzjN1bkhxeUWFHkg/OjF86rcpwQZKl6VaEW5NcWFWnTx9MuzDJrdO2B6vqgmkVhktXvNeRzgEAwAawLrcxVNV7kzwvyRlVtT/Lqyq8NcnNVXVZknuSvGTa/cNJLk6yL8m3k7wqScYYB6vq6iR3Tvu9ZYxx+ENvO7O84sPjk3xkeuQo5wAAYAOo5QUMNq6lpaW5/wOcf617dqG7PTu3znsKAO0tLCzUyjHfoAYAQFtiFwCAtsQuAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbELAEBbYhcAgLbELgAAbYldAADaErsAALQldgEAaEvsAgDQltgFAKAtsQsAQFtiFwCAtsQuAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbELAEBbYhcAgLbELgAAbYldAADaErsAALQldgEAaEvsAgDQltgFAKAtsQsAQFtiFwCAtsQuAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbELAEBbYhcAgLbELgAAbYldAADaErsAALQldgEAaEvsAgDQltgFAKCtucduVX2tqr5QVZ+rqs9MY0+pqt1VtXf6efo0XlV1TVXtq6rPV9WzZ95nx7T/3qraMTP+nOn9903H1vr/lgAAzMPcY3fyC2OMZ40x/tH0+ookt40xFpPcNr1OkouSLE6P1yS5LlmO4yRXJnlukq1JrjwcyNM+r545btva/zoAAJwMTpbYXWl7kl3T811JLpkZv3Esuz3Jk6vqrCQvSrJ7jHFwjHEoye4k26ZtTxpj3D7GGElunHkvAACaOxlidyT5aFX9RVW9Zho7c4xx3/T860nOnJ6fneTemWP3T2NHG99/hHEAADaAzfOeQJKfH2McqKqfSrK7qv7n7MYxxqiqsR4T2bt373qcBtiA/H0BWBuLi4tH3T732B1jHJh+PlBVH8jyPbf3V9VZY4z7plsRHph2P5Dk3JnDz5nGDiR53orxP5vGzznC/kd0rH+sNbP7jvmcF1g3c/v7ArDBzfU2hqr68ap64uHnSS5M8sUktyQ5vKLCjiQfnJ7fkuTSaVWGC5IsTbc73Jrkwqo6ffpg2oVJbp22PVhVF0yrMFw6814AADQ37yu7Zyb5wLQa2OYk/3mM8V+r6s4kN1fVZUnuSfKSaf8PJ7k4yb4k307yqiQZYxysqquT3Dnt95YxxsHp+c4k707y+CQfmR4AAGwAtbxIwca1tLQ093+A8691GwN0t2fn1nlPAaC9hYWFH/o+hZNhNQYAAFgTYhcAgLbELgAAbYldAADaErsAALQldgEAaEvsAgDQltgFAKAtsQsAQFtiFwCAtsQuAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbELAEBbYhcAgLbELgAAbYldAADaErsAALQldgEAaEvsAgDQltgFAKAtsQsAQFtiFwCAtsQuAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbELAEBbYhcAgLbELgAAbYldAADaErsAALQldgEAaEvsAgDQltgFAKAtsQsAQFtiFwCAtsQuAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG2JXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbELAEBbGyJ2q2pbVX2pqvZV1RXzng8AAOujfexW1aYk70xyUZItSV5WVVvmOysAANZD+9hNsjXJvjHGV8cY301yU5Ltc54TAADrYPO8J7AOzk5y78zr/UmeO6e5HNGenVvnPQUAgJY2wpVdAAA2qI0QuweSnDvz+pxpDACA5mqMMe85rKmq2pzky0lekOXIvTPJy8cYe+Y6MQAA1lz7e3bHGA9X1eVJbk2yKckNQhcAYGNof2UXTjZVtS3J72f5f77+YIzx1jlPCeCEqKobkvxikgfGGD897/lAsjHu2YWThnWfgebenWTbvCcBs8QurC/rPgNtjTE+keTgvOcBs8QurK8jrft89pzmAgDtiV0AANoSu7C+rPsMAOtI7ML6ujPJYlWdV1WnJXlpklvmPCcAaEvswjoaYzyc5PC6z3cnudm6z0AXVfXeJJ9K8g+qan9VXTbvOYF1dgEAaMuVXQAA2hK7AAC0JXYBAGhL7AIA0JbYBQCgLbEL0EBVXVVV/2ne8wA42YhdgFNIVb28qj5TVX9TVfdV1Ueq6ufnPS+Ak9XmeU8AgONTVW9IckWSX83yF5N8N8m2JNuTfGuOUwM4abmyC3AKqKqFJG9J8roxxvvHGN8aY3xvjPGnY4x/eYT9/7iqvl5VS1X1iao6f2bbxVV1V1U9VFUHqurXp/EzqupDVfXNqjpYVZ+sKv+dAE5p/ogBnBp+NsnjknzgOPf/SJLFJD+V5LNJ3jOz7fokrx1jPDHJTyf52DT+xiT7k/xkkjOTvCmJr9kETmluYwA4NTw1yV+PMR4+np3HGDccfl5VVyU5VFULY4ylJN9LsqWq/scY41CSQ9Ou30tyVpKnjTH2JfnkifwFAObBlV2AU8M3kpxRVce8SFFVm6rqrVX1lap6MMnXpk1nTD9/OcnFSe6pqj+vqp+dxv9dkn1JPlpVX62qK07srwCw/sQuwKnhU0m+k+SS49j35Vn+0NoLkywkefo0XkkyxrhzjLE9y7c4/JckN0/jD40x3jjGeEaSf5rkDVX1ghP3KwCsP7ELcAqYbj/4rSTvrKpLquoJVfXYqrqoqn5vxe5PzHIYfyPJE5L828Mbquq0qnrFdEvD95I8mOQH07ZfrKpnVlUlWUry/cPbAE5VYhfgFDHGeFuSNyR5c5L/neTeJJdn+ersrBuT3JPkQJK7kty+Yvsrk3xtusXhV5O8YhpfTPLfkvxNlq8kXzvG+PgJ/0UA1lGN4YO2AAD05MouAABtiV0AANoSuwAAtCV2AQBoS+wCANCW2AUAoC2xCwBAW2IXAIC2xC4AAG39X3XJI5vC9ddaAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(credit_data[\"Class\"].value_counts(normalize=False))  # highly imbalanced data\n",
    "\n",
    "plt.figure(figsize=(10, 9))\n",
    "sns.countplot(data=credit_data, x=\"Class\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dive into features level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAAG+CAYAAABlFvQZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA2cklEQVR4nO3debitZV038O9P0BRNQDQiILE8lViK1osYOVKIVqKmhDmg8mpvatrbqFmvpllpg1NqpSJoDpGKoqJEiAOZiMokDp3jCCfQFASVcrzfP55nw3KzhzOsvdZ99v58rmtda637Ge7vWus+z9nrt56hWmsBAAAA6NkN5h0AAAAAYDUKGAAAAED3FDAAAACA7ilgAAAAAN1TwAAAAAC6p4ABAAAAdE8BA4AdVlWfraq2yu0B8865q5t4Lw9aZb6Fz+OeU+r33dNc30ZUVc8Y38NnzDsLAOzqdp93AADWhdOTXL7MtM/PMkgyfGlM8vQkf9Jae8as+wfmayz2fSbJ51prB803DQDTooABwDT8RWvt3fMOwdQ9MskemUMRCgBgMQUMAGBJrTWFCwCgG86BAcBMVdV9qurUqvpCVX2zqi6rqtdV1U8tM//PV9WLq+qCqvpyVX2jqj5XVSdV1e2WmL9lOHwkSZ6+6HwczxjnOWh8/tkVcrZxXcu2V9XxVXVOVV09tu81Md/tquoVVfWZqvqfqrqyqv61qu6/HW/XVEycG+OgqvqFqjqzqq6qqmuq6gPLZVrpHBhVddOqenZVfWr8TC6pqpdU1T5VdeK43KO2dX3j9CWXm5i+XWNnmXUcNfZx3grz3GJ8Td+oqltMtG/XWFwlx4rnxqiqR43TT1xm+lTHV1XdsKoeV1VnVdUV42v7fFW9raoetsT8N62qp43vxdfH2/lV9YdVtccOvJ57jtPfvVz7mPFpVfWJ8TV/sar+sap+eNEyJ2Y4fCRJbr1oG/DZifluXFVPqaqPVNXXxtd8WVX9e1X9aVXdeHvfRwDWlj0wAJiZqnpBkicl+XaSc5NcmuS2SY5N8oCq+pXW2mmLFvu7JAckuTjJe8e2n8xweMODq+o+rbWzJ+Y/KckhSe6Y5IIk509Mm3y8s6/lRUken+TfkrwtyY8lWShsHDvmuNGY+21JbpXkbkmOqKpntdb+37SybIfjkzwtw3t/WpIfT3KXJG+uqmNaa2/YlpVU1U2TnJXkfyW5Osk7knwnw+d4ZIbXPFU7OHaWckaS/0xySFXdobV24RLzPDTDZ/fG1toVE+3bOxbXxLTHV1XtneTtSe6a5BsZxvQXk/xQksMzvMbXTMx/yyTvSvJTSa7McA6cJLlXkmcnOaaq7r3ovdtZN8wwzu6S5D1JPj7mfViSu4+f5VfGec9OcrMkv5Lk60kmx/WXxtdwg/E13zvJVeM6r0qyb4Z/F09L8rdZ/tw+AMxDa83Nzc3NzW2Hbkk+m+FL+z23Yd7/M8770SQ/sWjaA5J8K8OXob2XmLbXorZK8uvj+j6WpBZNf8Y47RnLZDlonP7ZFfK24b/JpduTfCXJoUtMv0OGL4FfTXLfRdNun+F8Ei3JvbbjfV7o86Ad+Twm2r+R5KhF0/5onLZ5ifW9e5n1/fXYfmGSH5ho3yvDl9+FvI/alvVNTD9xmeV2aOys8D79+bi+v1lm+gfH6b+0VmNxG8boo8bpJ85gfL15XOb9SX5o0bQbL9HPyeP87518P5LsPfH5v25bXs/E9HuO09+9THvLULiaHG97JvnwOO1pi5Y7KCv8G09y93H6h5PcdInP9PAke2zre+jm5ubmNpubQ0gAmIazFu2mvXA7MUmqarckC78IH9Na+8Tkwq21Nyf5+wxfgB++eFq77pfVhbbWWvv7DF+4bpfk4Om/pFU9t7X2wSXan5bhl/Hfb629Y3JCa+3iJL89Pn3iGudbyotaa+9c1PbcDL8833bxrvhLqaqbJHnc+PRJrbUvLkwbP6ffyLgnyjTszNhZwYnj/cOq6nv2Rq2qgzPsWXJ5ku95rzoZi1MdX1V1SJKjMxREjm6t/eeidf7PZD9VdeskD07y3SSPnXw/WmtXJnnsOO2Yqjpwu17ZylqSxywab1clec749IjtXN++4/37Wmtf/56OBv/WWrtmh9MCsCYcQgLANCx3GdWF3ekPSbJfkotbax9bZh3vSfKEDLuFv2hyQlUdkOQXk/xEkpsn2W2c9IPj/Y9lDQ5bWMWbFjeMu6UfleHL1nKHY7xnvL/rGuVaydsWN7TWvllVn05ypwyHDKx24s6fzrB7/ta2xJVnWmsXVtWFGQ7hmYZDshNjZymttU9W1QeSHJbkfklOnZh83Hj/mtbatxcvO8+xuEbj66jx/tTW2n9tw/x3y7CHwr+31j65eGJr7WNVdc7Y/90zcejJTvp8a+2iJdoXClo/tJ3r+0iGw56Or6r/yHC40Bd2JiAAa08BA4BpWO0yqj8y3t++ljgx5iK3mnxSVX+S5A+z8v9ZN1814fR9bom2fXJdli9W1UrL32qlictYcYUT05d7j5crTlw93m/LSQsPGO8/s8I8n830Chg7PHZW8coMBYzjMhYwxr09FvbiOHHxAh2MxbUYX7ce7z+x4lzX2X+8X+nz/3SGAsb+K8yzvaYxdq/VWvtUVf3fJH+V5MVJXjwW8t6f5C1JTmmtfWdHwwKwNhQwAJiFhV+ptyb511XmvfaLVFX9SobDB76aYdf4dyW5rLX23+P012Y44eJqX+y3y/hL94oWMiyy8Dq/k+QfpxjpmiR7JLnpKvPdbLz/2jLTvzu1RGtjqfd9h8bONvinJM9P8ktVtU9r7ctJfj7DL/kfbq19dHLmOYzFld6LaY6vHT3cZ2qHCY1W+zc39bHbWntRVf1zhnOb/Nx4e/h4O7+q7tFau3qFVQAwYwoYAMzCJeP9Za21R23Hcg8Z7/+wtfbyJabfdgfzfHO8v9ky02+9TPtqvpTkv5PcJMkTW2vLFRK21+czHLJw2wwnsryeGi73uXDJz0uWmmdKto73B60wz3LTduR939Gxs6LW2lVV9eYMRYdfy3DoycL6T1xikWmPxR15L9ZifC3s2fDj2zj/wuf/IyvMszBt60TbWv2b2ymttcszXF3m75Kkqu6Y5NUZDl16SoY9bgDohJN4AjALH0zy5SR3qqrt+aK37BfyqrpdhvM2LGXhy9Jyhfr/GufZp6qW2tX+ftuR8VrjORMW9hJ48I6sYxlnjfe/ssI8C/19bPJEh2vgwxkuTXlAVd198cSq+skMV8pYysIX2p9YYrl9k9x5iWV2dOxsi1eO98dV1Z4Zfon/ZpLXLjHvjo7F5az0XlSuOzfFtdZofC1cAvXo8fKoq3lfhr0vDquqH1s8cXwv7pJhj4n3Tkxa9vWOdujf3ApW2wYsqbV2QZIXjE+ndRgUAFOigAHAmmutfSvJszLsAv/mqjp08TxVdaOqun9VTX7BWTgk4LFVdaOJeX8gyUlZ/svJwpel262Q533j0z+piZMJVNXPJXnm6q9qWc/McFnPF1TVsZPrHtdfVXVoVR25Het8YYZLZz6sqo5fPLGq7prkz8anf7mDubfJeGWGhT0QXjBZABqLAC/J8odRnDneP6Gq9ptY7hYZPs/r/Tq/E2NnW5yZoSDx0xk+txsneWtr7Yol5t3RsbicszJ8yT+qqg6fWN9uSZ6d5HqvczTV8dVaOy/JW5N8f5JTJj+XcX03rqr7Tsz/uSRvzPA35N+Pn/nCvHtluCLMDZKc3FqbLPacm+Hwm9tX1UMX9fH4TLfgl1xXpNy3qvZePLGq7l1V91viKjS75bpiylLnuQFgjhQwAJiJ1toLkjwvye2TnFNVF1TVm6rq9VX1viRXZDh53kETiz0/wyU+fzHJlqr656p6W5JPZfiy++Zlujs9w3kjHlRV762qV1bVy6vq/hPz/L8MX3B+I8nF47o/mOEqDi/Zidf5oSSPTHLDJK9L8pmqOq2q/rGqFq7Wck6Se2/HOj+R5DEZvri+vKo+NeZ9XVWdm+TfMpzg8W9bayfuaPbt8EcZruJwSIbP5ZSqekOGkzfun++9qsekk5Ocl+Ezvriq3jq+J1synBz0zUsttINjZ1Wtte9mOFwgSZ403p+4zOzPz46NxeX6/nySl2YozJxVVWdW1SkZ3sMnZihaLbXc1MdXhkNnzs1wDohPV9UZVfXaqnp3ksvGnJN+I8OhTPcc539jVb1xzH63JBdkuCrMZO5rcl1h8DVVdXZVvaGqPpnhvX3uduRd1Vj4enuGwtJ5VfWacRvwF+Msdxinf6mq3jVOPyVDQetBGd7H5yy1bgDmRwEDgJlprf12knskeX2SvTN8GTwqyS0zXOLzYbluz4i01hYu7/n6DL/q/3KGvSr+IcNVDq5app/Lk/xSkndn+KJyXJLjM3GIQmvt/UmOyPAr/IG57lfXR7bW/ngnX+frk/xUhi+h14yv+YEZzpNwfpInZ5kvqCus87UZCgZ/n+TbY94HZbh855uS3Ke19ps7k3s7snwtw2v6iwzFg/tluKLHGzIcPnDlMst9M8OJMl+a4VwO98lwSMFJSX42y3ye47LbNXa2w4kTjy9P8s5l+t+hsbiKJyX5gwxf/BdOInlOkp/JUOhZ0rTH17jHyd2S/GaGwtShGcbWbTK8p09ZNP+XMrzmP86wt9N9x9slSZ6W5PCl9mJprf1Vhn+HF46v8YgMBaCfS/KObc27HR6b5BUZikTHjH0fO057a5I/yfB6b5vh8Ky7ZRgDT09yh3FvEwA6Uq1N+yTSAMBGVlUnZigaPXpGe4QAABuAPTAAAACA7ilgAAAAAN1TwAAAAAC6t0ufA+Oqq67adcMDAAAAy9pzzz2/53Lh9sAAAAAAuqeAAQAAAHRPASPJ5s2b5x1h7hnm3b8MMvSWYd79yyBDT/3LIENP/csgQ28Z5t2/DDL01P9aZ1DAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuVWtt3hl22FVXXTWV8Js3b86mTZumsaol7fXKrTu9jq88ev8Nn2Fn+5ehnwzrYTz2kGE9jIUeMqyHsdBDhvUwFnrIsB7GQg8Z1sNYkGE6/cswnf5lmE7/08iwmrX+bjvrDHvuuWdNPrcHBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDurVrAqKoDq+qsqvpYVV1cVU8e259RVVur6vzxdr+JZZ5aVVuq6pNVdZ+J9qPGti1V9ZSJ9ttU1Tlj+z9V1Y3G9u8bn28Zpx801VcPAAAA7BK2ZQ+Mbyf5ndbawUkOS/KEqjp4nPa81toh4+20JBmnHZvk9kmOSvKSqtqtqnZL8uIk901ycJKHTqznOeO6bpvkyiTHj+3HJ7lybH/eOB8AAACwwaxawGitXdZa+8j4+KtJPp5k/xUWOTrJ61tr32itfSbJliSHjrctrbVPt9a+meT1SY6uqkpy7yRvGJc/KckDJtZ10vj4DUmOGOcHAAAANpDtOgfGeAjHnZKcMzY9saourKoTqmrvsW3/JJdMLHbp2LZc+z5JvtJa+/ai9u9Z1zj9qnF+AAAAYAPZfVtnrKqbJXljkt9qrV1dVS9N8qwkbbz/6ySPWZOU22Dz5s1zXX5le+z0GnY+366fYTqfkQx9ZNj1x2MPGdbHWOghw64/FnrIsD7GQg8Zdv2x0EOG9TEWZJhG/zJMq38ZptH/dDL00cdaZti0adOy07apgFFVN8xQvHhNa+1NSdJa+8LE9Jcledv4dGuSAycWP2BsyzLtX06yV1XtPu5lMTn/wrourardk+w5zn89K73I1WzevHmnll/V2VtXn2cVO51vHWSYymckQx8Z1sF47CHDuhgLPWRYB2OhhwzrYiz0kGEdjIUeMqyLsSDDVPqXYUr9yzCV/qeSYRVr/t12zhm25SokleQVST7eWvubifb9JmZ7YJKPjo9PTXLseAWR2yTZlOSDSc5Nsmm84siNMpzo89TWWktyVpIHj8sfl+QtE+s6bnz84CTvGucHAAAANpBt2QPj8CSPSHJRVZ0/tv1hhquIHJLhEJLPJvn1JGmtXVxVJyf5WIYrmDyhtfadJKmqJyY5PcluSU5orV08ru8Pkry+qv40yXkZCiYZ719dVVuSXJGh6AEAAABsMKsWMFprZydZ6sofp62wzLOTPHuJ9tOWWq619ukMVylZ3P4/SR6yWkYAAABgfduuq5AAAAAAzIMCBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALq3agGjqg6sqrOq6mNVdXFVPXlsv0VVnVFVm8f7vcf2qqoXVtWWqrqwqu48sa7jxvk3V9VxE+0/XVUXjcu8sKpqpT4AAACAjWVb9sD4dpLfaa0dnOSwJE+oqoOTPCXJma21TUnOHJ8nyX2TbBpvj0vy0mQoRiR5epK7JDk0ydMnChIvTfLYieWOGtuX6wMAAADYQFYtYLTWLmutfWR8/NUkH0+yf5Kjk5w0znZSkgeMj49O8qo2+ECSvapqvyT3SXJGa+2K1tqVSc5IctQ47eattQ+01lqSVy1a11J9AAAAABvI7tszc1UdlOROSc5Jsm9r7bJx0uVJ9h0f75/kkonFLh3bVmq/dIn2rNDH9WzevHl7XsrUl1/ZHju9hp3Pt+tnmM5nJEMfGXb98dhDhvUxFnrIsOuPhR4yrI+x0EOGXX8s9JBhfYwFGabRvwzT6l+GafQ/nQx99LGWGTZt2rTstG0uYFTVzZK8MclvtdauHk9TkSRprbWqajuccBus1sdKL3I1mzdv3qnlV3X21p1exU7nWwcZpvIZydBHhnUwHnvIsC7GQg8Z1sFY6CHDuhgLPWRYB2OhhwzrYizIMJX+ZZhS/zJMpf+pZFjFmn+3nXOGbboKSVXdMEPx4jWttTeNzV8YD//IeP/FsX1rkgMnFj9gbFup/YAl2lfqAwAAANhAtuUqJJXkFUk+3lr7m4lJpyZZuJLIcUneMtH+yPFqJIcluWo8DOT0JEdW1d7jyTuPTHL6OO3qqjps7OuRi9a1VB8AAADABrIth5AcnuQRSS6qqvPHtj9M8hdJTq6q45N8Lskx47TTktwvyZYk1yR5dJK01q6oqmclOXec75mttSvGx49PcmKSmyR5x3jLCn0AAAAAG8iqBYzW2tlJapnJRywxf0vyhGXWdUKSE5Zo/1CSn1yi/ctL9QEAAABsLNt0DgwAAACAeVLAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACge6sWMKrqhKr6YlV9dKLtGVW1tarOH2/3m5j21KraUlWfrKr7TLQfNbZtqaqnTLTfpqrOGdv/qapuNLZ/3/h8yzj9oKm9agAAAGCXsi17YJyY5Kgl2p/XWjtkvJ2WJFV1cJJjk9x+XOYlVbVbVe2W5MVJ7pvk4CQPHedNkueM67ptkiuTHD+2H5/kyrH9eeN8AAAAwAa0agGjtfbeJFds4/qOTvL61to3WmufSbIlyaHjbUtr7dOttW8meX2So6uqktw7yRvG5U9K8oCJdZ00Pn5DkiPG+QEAAIANZmfOgfHEqrpwPMRk77Ft/ySXTMxz6di2XPs+Sb7SWvv2ovbvWdc4/apxfgAAAGCD2X0Hl3tpkmclaeP9Xyd5zLRC7YjNmzfPdfmV7bHTa9j5fLt+hul8RjL0kWHXH489ZFgfY6GHDLv+WOghw/oYCz1k2PXHQg8Z1sdYkGEa/cswrf5lmEb/08nQRx9rmWHTpk3LTtuhAkZr7QsLj6vqZUneNj7dmuTAiVkPGNuyTPuXk+xVVbuPe1lMzr+wrkuravcke47zL2mlF7mazZs379Tyqzp76+rzrGKn862DDFP5jGToI8M6GI89ZFgXY6GHDOtgLPSQYV2MhR4yrIOx0EOGdTEWZJhK/zJMqX8ZptL/VDKsYs2/2845ww4dQlJV+008fWCShSuUnJrk2PEKIrdJsinJB5Ocm2TTeMWRG2U40eeprbWW5KwkDx6XPy7JWybWddz4+MFJ3jXODwAAAGwwq+6BUVWvS3LPJLesqkuTPD3JPavqkAyHkHw2ya8nSWvt4qo6OcnHknw7yRNaa98Z1/PEJKcn2S3JCa21i8cu/iDJ66vqT5Ocl+QVY/srkry6qrZkOInosTv7YgEAAIBd06oFjNbaQ5dofsUSbQvzPzvJs5doPy3JaUu0fzrDVUoWt/9Pkoeslg8AAABY/3bmKiQAAAAAM6GAAQAAAHRPAQMAAADongIGAAAA0D0FDAAAAKB7ChgAAABA9xQwAAAAgO4pYAAAAADdU8AAAAAAuqeAAQAAAHRPAQMAAADongIGAAAA0D0FDAAAAKB7ChgAAABA9xQwAAAAgO4pYAAAAADdU8AAAAAAuqeAAQAAAHRPAQMAAADongIGAAAA0D0FDAAAAKB7ChgAAABA9xQwAAAAgO4pYAAAAADdU8AAAAAAuqeAAQAAAHRPAQMAAADo3u7zDjALe71y6ypz7JGcvfw8X3n0/tMNBAAAAGwXe2AAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0TwEDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPcUMAAAAIDuKWAAAAAA3VPAAAAAALqngAEAAAB0b9UCRlWdUFVfrKqPTrTdoqrOqKrN4/3eY3tV1QuraktVXVhVd55Y5rhx/s1VddxE+09X1UXjMi+sqlqpDwAAAGDj2ZY9ME5MctSitqckObO1tinJmePzJLlvkk3j7XFJXpoMxYgkT09ylySHJnn6REHipUkeO7HcUav0AQAAAGwwqxYwWmvvTXLFouajk5w0Pj4pyQMm2l/VBh9IsldV7ZfkPknOaK1d0Vq7MskZSY4ap928tfaB1lpL8qpF61qqDwAAAGCD2X0Hl9u3tXbZ+PjyJPuOj/dPcsnEfJeObSu1X7pE+0p9LGnz5s0rTN1jpUVXtfK6t8XO9S/DtPqXoZ8Mu/547CHD+hgLPWTY9cdCDxnWx1joIcOuPxZ6yLA+xoIM0+hfhmn1L8M0+p9Ohj76WMsMmzZtWnbajhYwrtVaa1XVdnY9O9vHSi8yZ2/dqf5XXPe22Mn+ZZhS/zL0k2EdjMceMqyLsdBDhnUwFnrIsC7GQg8Z1sFY6CHDuhgLMkylfxmm1L8MU+l/KhlWsXnz5jXvY54ZdvQqJF8YD//IeP/FsX1rkgMn5jtgbFup/YAl2lfqAwAAANhgdrSAcWqShSuJHJfkLRPtjxyvRnJYkqvGw0BOT3JkVe09nrzzyCSnj9OurqrDxquPPHLRupbqAwAAANhgVj2EpKpel+SeSW5ZVZdmuJrIXyQ5uaqOT/K5JMeMs5+W5H5JtiS5Jsmjk6S1dkVVPSvJueN8z2ytLZwY9PEZrnRykyTvGG9ZoQ8AAABgg1m1gNFae+gyk45YYt6W5AnLrOeEJCcs0f6hJD+5RPuXl+oDAAAA2Hh29BASAAAAgJlRwAAAAAC6p4ABAAAAdE8BAwAAAOieAgYAAADQPQUMAAAAoHsKGAAAAED3FDAAAACA7ilgAAAAAN1TwAAAAAC6p4ABAAAAdE8BAwAAAOieAgYAAADQPQUMAAAAoHsKGAAAAED3FDAAAACA7ilgAAAAAN1TwAAAAAC6p4ABAAAAdE8BAwAAAOieAgYAAADQPQUMAAAAoHsKGAAAAED3FDAAAACA7ilgAAAAAN1TwAAAAAC6p4ABAAAAdE8BAwAAAOieAgYAAADQPQUMAAAAoHsKGAAAAED3FDAAAACA7ilgAAAAAN1TwAAAAAC6p4ABAAAAdE8BAwAAAOieAgYAAADQPQUMAAAAoHsKGAAAAED3FDAAAACA7ilgAAAAAN1TwAAAAAC6p4ABAAAAdE8BAwAAAOieAgYAAADQPQUMAAAAoHsKGAAAAED3FDAAAACA7ilgAAAAAN1TwAAAAAC6p4ABAAAAdE8BAwAAAOieAgYAAADQPQUMAAAAoHs7VcCoqs9W1UVVdX5VfWhsu0VVnVFVm8f7vcf2qqoXVtWWqrqwqu48sZ7jxvk3V9VxE+0/Pa5/y7hs7UxeAAAAYNc0jT0w7tVaO6S19jPj86ckObO1tinJmePzJLlvkk3j7XFJXpoMBY8kT09ylySHJnn6QtFjnOexE8sdNYW8AAAAwC5mLQ4hOTrJSePjk5I8YKL9VW3wgSR7VdV+Se6T5IzW2hWttSuTnJHkqHHazVtrH2ittSSvmlgXAAAAsIHsbAGjJfmXqvpwVT1ubNu3tXbZ+PjyJPuOj/dPcsnEspeObSu1X7pEOwAAALDB7L6Ty/9ca21rVf1AkjOq6hOTE1trraraTvaxTTZv3rzC1D3WcN3bYuf6l2Fa/cvQT4Zdfzz2kGF9jIUeMuz6Y6GHDOtjLPSQYdcfCz1kWB9jQYZp9C/DtPqXYRr9TydDH32sZYZNmzYtO22nChitta3j/Rer6pQM57D4QlXt11q7bDwM5Ivj7FuTHDix+AFj29Yk91zU/u6x/YAl5l/SSi8yZy+72DZZcd3bYif7l2FK/cvQT4Z1MB57yLAuxkIPGdbBWOghw7oYCz1kWAdjoYcM62IsyDCV/mWYUv8yTKX/qWRYxebNm9e8j3lm2OFDSKrqplX1/QuPkxyZ5KNJTk2ycCWR45K8ZXx8apJHjlcjOSzJVeOhJqcnObKq9h5P3nlkktPHaVdX1WHj1UceObEuAAAAYAPZmT0w9k1yynhl092TvLa19s6qOjfJyVV1fJLPJTlmnP+0JPdLsiXJNUkenSSttSuq6llJzh3ne2Zr7Yrx8eOTnJjkJkneMd4AAACADWaHCxittU8nueMS7V9OcsQS7S3JE5ZZ1wlJTlii/UNJfnJHMwIAAADrw1pcRhUAAABgqhQwAAAAgO4pYAAAAADdU8AAAAAAuqeAAQAAAHRPAQMAAADongIGAAAA0D0FDAAAAKB7ChgAAABA9xQwAAAAgO4pYAAAAADdU8AAAAAAuqeAAQAAAHRPAQMAAADongIGAAAA0D0FDAAAAKB7ChgAAABA93afdwAAAABgdXu9cusqc+yRnL3yPF959P7TCzRj9sAAAAAAuqeAAQAAAHRPAQMAAADongIGAAAA0D0FDAAAAKB7ChgAAABA9xQwAAAAgO4pYAAAAADdU8AAAAAAuqeAAQAAAHRPAQMAAADongIGAAAA0D0FDAAAAKB7ChgAAABA9xQwAAAAgO4pYAAAAADdU8AAAAAAuqeAAQAAAHRPAQMAAADongIGAAAA0D0FDAAAAKB7ChgAAABA9xQwAAAAgO4pYAAAAADdU8AAAAAAuqeAAQAAAHRPAQMAAADongIGAAAA0D0FDAAAAKB7ChgAAABA9xQwAAAAgO4pYAAAAADdU8AAAAAAuqeAAQAAAHRPAQMAAADongIGAAAA0D0FDAAAAKB73RcwquqoqvpkVW2pqqfMOw8AAAAwe10XMKpqtyQvTnLfJAcneWhVHTzfVAAAAMCsVWtt3hmWVVV3TfKM1tp9xudPTZLW2p8nyVVXXdVveAAAAGCH7bnnnjX5vOs9MJLsn+SSieeXjm0AAADABtJ7AQMAAAAgu887wCq2Jjlw4vkBY1uS6+9OAgAAAKxPve+BcW6STVV1m6q6UZJjk5w650wAAADAjHW9B0Zr7dtV9cQkpyfZLckJrbWL5xwLAAAAmLGur0ICAAAAkPR/CMmaqKp9q+oVVfWO8fnBVXX8vHMBAAAAS9uQBYwkJ2Y4LOWHxuf/keS35hVmQVX9wgz7unlV/egS7XeYUf8/WFU/OD6+VVU9qKpuP4u+V8j0Z3Pu/zbj+/ATM+zzh6vqxuPjqqpHV9WLquo3qmomh5hV1f0XMsxLVd29qn58fHx4Vf1uVf3ijDPcrKoeXFX/t6qeVFVHVdXMttFVtXtV/XpVvbOqLhxv76iq/1NVN5xVjhXy/cOM+tltfB+eVVWHL5r2RzPof4+q+v2q+r2qunFVPaqqTq2q51bVzda6/xVy/ceM+7vDxOMbVtUfje/Dn1XVHjPK8MSquuX4+LZV9d6q+kpVnVNVPzWjDG+qqofP+bP/kao6oar+dNxOvayqPlpV/1xVB82g/xtU1WOq6u1VdUFVfaSqXl9V91zrvicy2D5m/tvHsZ/utpG2j7aP89o+TuQ4c1vaZpDjyTV8x6wadhb4SFUdOfV+NuIhJFV1bmvtf1XVea21O41t57fWDplzrs+31n54Bv0ck+T5Sb6Y5IZJHtVaO3ec9pHW2p3XuP9fT/KUJJXkOUkeleSjSX4uyXNba69Yy/7HDC9c3JTkEUlelSSttSfNIMObW2sPGB8fneEzeXeSn03y5621E2eQ4aNJDm2tXVNVz0nyo0nenOTeSdJae8wMMvx3kq8neUeS1yU5vbX2nbXud6L/5yc5NMM5gU5PcsSY5R5Jzmut/d4MMhyT5HeTXJjkXknen6HA/FNJHtZau2gGGV6X5CtJTkpy6dh8QJLjktyitfarM8hwi+UmJbmgtXbADDK8PMkeST6YYZvwntbab4/TZrF9PDnJJUlukuTHk3w8yT8luX+SH2ytPWIt+x8zfDXJwh8HC1f72iPJNUlaa+3mM8hw7XtdVX+dZJ8kr0zygCT7tNYeOYMMF7fWbj8+fnuSl7fWThm/OD+7tXb4SstPKcPWJP+eYZv8rxm2kW9vrX1zrfueyPDesd89kzw8w+dwcpIjM2yf7r3G/b8yyecyvP4HJ7k6yfuS/EGSt7TWXrSW/Y8ZbB8z/+3j2M9ct5G2j9dmsH1MF9vHG2cYf2cluWeuG5M3T/LO1trMfhQd81zQWrtjVd0nya8n+eMkr576tqG1tuFuGb4k7pPkI+PzwzJshGfR96nL3N6a5OszynB+kv3Gx4cm+USSB47Pz5tB/xdl+Me2T5KvZfgPJ0n2TnL+jN6DS5L8Y5JHZvgD5Lgk/7XweEYZzpt4/P4ktxkf3zLDHyOzyPCxiccfTnKDieezynDe+Nk/NsmZSb6Q5O+S3GNG/V+cYYO/R5Irk+wxtt8wyUdnlOHCiX5vmaGIkyR3SPL+GWX4jx2ZNuUM30ny6SSfmbgtPP/mrD6Lice7J/mHJG9K8n0z2j6eP95Xkstz3Q8NNZltjTO8MEMxd9+Jts/Mou+J/s6beHx+khvO4X345MTjc5cbJ7N4HzL8MfqIJKeN/1e9MsmRc/gsPr/ctDXs/8JFzz8w3n9fko/P6D2wfWzz3z6O/Z4/3s9lG2n7eG2/to/X/yzmsX188rgN+Mai7cMFSZ44i/dgqc8+yQuyht8tu74KyRr67QxFgx+tqn9LcqsMVf1ZuFuGCt3XFrVXhmLCLOzeWrssSVprH6yqeyV5W1UdmOuqymvp2621a5JcU1Wfaq1dPma5sqpmtUvQ7ZM8M8lRSX63tfafVfX01tpJM+o/+d73evfW2meSpLX2par67owyXFJV926tvSvJZ5McmORzVbXPjPpPhl8srkzysiQvq+HQomOS/EVVHdBaO3AG/beJ93zhc/luZneYXSX57/Hx15P8wBjswqpa819zRldU1UOSvLG19t1k2HU7yUMyFHZm4dNJjmitfX7xhKq6ZEYZbrTwoLX27SSPq6r/l+RdSWa2m+o4Jk9r4//+4/OZbB9ba0+qqp9O8rqqenOSv81s/m+YtGdVPTDDv8Hva619a8w2s/chyRuq6sQM/1ecUlW/leSUDL/2XW+MrpGFz//qJK9O8upx+/yQDHsy/ssMMny3qn4swy+Me1TVz7TWPlRVt81whbi19q2q+tHW2qeq6s5JvpkkrbVvzHAs2D4Outg+jv3PZRtp+3gt28fBXLePrbUXJHlBVf1mm8HeaNvgw1X1L0luk+SpVfX9Gf6enqoNWcBorX2kqu6RYdezylBF/NaMuv9Akmtaa+9ZPKGqPjmjDFcv/DGQJK21y8Zdvt6c4Yv9WvtuVd1wfM+vPc/AuBvUTL4wjhu73xr/E3rNuPvbrM8Jc4equjrDGLxxVe03fhY3ymz+KEyS/53kVVX1jCRXJTm/qs5PsleGQt/MjQWtFyZ5YVXdegZdvr2qzs7wC9LLk5xcVR/IcAjJe2fQf5K8Pck7x10Rj0ryz8m1uwzXSgtO0bEZDul6SVUt/EG+V4bdEo+dUYbnZ9gbZ6k/fp47owwfqqqjWmvvXGhorT2zqv4zyUtn1P/NWmtfaxOHcNVwzqKvzqD/JElr7cNV9fNJnpjkPUlmfZ6a92TYJTxJPlBV+7bWvjAWOL80iwCttadV1aMy7B78oxm2EY/L8H/lw2aRIdf/sSOttS9n2Evt72aU4fcz7CX63Qy7qD+1qu6Y4VfPx86g/99LclZVfSPD363HJsP5s5K8bQb9J7aPC+a9fVzIMNdtpO2j7eOEeW8fkySttRdV1c8mOSgT3+9ba6+aVYbR8UkOSfLpNhyefoskj552Jxv1HBi7ZfjifFC+90P+mxn0/ZIkr22tnb3Wfa2Q4bQkf7Y4Qw0nojqmtfaaNe7/hCSvaK3926L2/ZPcrrX2r2vZ/9jXizN8Dv9WVZXk8Unu2lp7+Fr3PZFhybFQVXtleB/+fQYZXpzhP58rkmzK8O/h0gy7A85kL5Cq+liSxy4eD7Oy8Dkk+VZr7Zzxj6AHZvgj8Q2zeB/GDJdlOIb2goV/A+MvfDdsrX1jrTMsyrNPcu0fAnSiqqrN4T/tqtovyZ1aa6fNum/6VMPJ+65sMzpf0fj/9D6ttZl8OVsli+1jp+axjbR9ZLFZbx/HPl+doZB0fobDzZJhp5w1P6ffohyHZzjM6+tV9fAkd07ygtba56bZz0a9CslbM5w4cp8k3z9xm4VPJvnLqvpsDWdMvtOM+p10+lIZWmvfWuvixeiCJH+1RP9bZ1G8GP3HQoYMv6q8f5bFi9GSY6G19pVZFC9G/5HkLzMcN3h4horpObMqXoz+PkuMhxn6ZIb34J+q6rlJbt5a+6vW2skzfB8+meR+SZ6U5MiJsfDdWRcvxn6/PPnHec3wCknLkSFJ8vPz6LS1dtnCH+cdvAcydJChtfal1tp3ZpWhDa5XvJjle1Dj1duW2D7O5OptkxmWaJdhMKurX1zb/6Lt49zfAxnmn2Fi+zizDEl+JsnhrbXHt9Z+c7zNtHgxemmGUwTcMcnvJPlUxgskTNNG3QPjwtbaLAfVUhlunWG3w2MznEn5dUle11qb2aWYlsnw2tba5jn238N7IMMGzDDv/nvJsEyumVwhSYa++5dBht4yzKr/mvPV22ToJ8O8+5dBhmVy/HOSJ7XxHIfzsvCaazg3ztbW2ivW4n3YqAWM5yQ5s7U2i5O7rGr8tfWEJHdorc3q3AddZZh3/zLI0FP/88hQVacuNynJvVtrN5Vh7TPMu38ZZOgtw7z7HzOcn+S+bThP1aEZflF8ahsuG3lea23N9xyUoY8M8+5fBhmWyXFWhnNPfDDDFUmSJK21+y+3zBrleE+Sd2Y478XdMxR2LmitTXXvqA15Es8MJ9I8pYbjy7+V4T/B1mZw7eYFVbV7kvtm+KX1iAyXdn3GrPrvIcO8+5dBhp767yBDD1dIkmH+/csgQ28Z5t1/Mv+rt8nQT4Z59y+DDEt5xgz7WsmvJvm1JMe31i6vqh/OcJj2VG3UAsbfJLlrkovajHdBqeF4zYdmON79g0len+RxrbWvb5QM8+5fBhl66r+XDOnjCkkyzL9/GWToLcO8+0/mf/U2GfrJMO/+ZZDhepbaPs5DG64k+DcTzz+fNTgHxkYtYFyS5KOzLl6Mnprhige/01qb1bXDe8sw7/5lkKGn/nvJ8JkMe6RdT2vt7jLMLMO8+5dBht4yzLv/JPlKkv0ynJBuoe+vVtVRSY6RYUNlmHf/MshwPVX11Vy3x8eNMpyP4+uzPLpgzHFYkhclud2YY7ckX2ut7TnVfubzHX6+qurEJD+S5B353uOE1vwyqgA9qqonZzh0Zb8kJ2c4eeh5Msw2w7z7l0GG3jLMu38ZZOipfxlk2IZMleToJIe11p4y474/lOH9+OcMV0Z5ZJIfa609dar9bNACxtOXam+t/cmsswD0pDq4EooM8+9fBhl6yzDv/lfIMLOrt8nQT4Z59y+DDNuQ6bw2o5OITvT5odbaz9TEFT/XIseGLGAAsLragFdj6THDvPuXQYbeMsy7fxlk6Kl/GWSoqgdNPL1Bhr0f7tFau+ss+p/I8d4kP5/k5UkuT3JZhkvL3nGa/dxgmivrXVX97Xj/1qo6dfFt3vkA5q2qdq+qX66q12Q4zO6TSR60ymIyrLP+ZZChtwzz7l8GGXrqXwYZFvnlidt9knw1w2Eks/aIDOe9eGKSryc5MMmvTLuTDbUHRlVd3Vq7eVXdY6npvZzBFWDWaukrobylzf9qLBsqw7z7l0GG3jLMu38ZZOipfxlkYOMVMGZ+LBDArqCq3pXhSihvnNeVUGSYf/8yyNBbhnn3L4MMPfUvgwzL5Dggw9U/Dh+b3pfkya21S2fU/0W57ioo17NwPoyp9bfBChiXZuLatIu5CgkAAAC7iqo6I0Mh5dVj08OTPKy19gsz6n9Tkn2TXLJo0oFJLm+tbZlmfxvqHBgZjsm5WZLvX+YGAAAAu4pbtdZe2Vr79ng7McmtZtj/85Jc1Vr73OQtyVXjtKnafdor7NxlrbVnzjsEAAAATMGXq+rhGS7fmgzn5fjyDPvft7V20eLG1tpFVXXQtDvbaHtg1LwDAAAAwJQ8Jskxue7SpQ9O8ugZ9r/XCtNuMu3ONto5MG7RWrti3jkAAABgV1dVr0vyrtbayxa1/+8kv9Ba+9Wp9reRChgAAACwXlTVbZL8ZpKDMnGKiNba/WfU/75JTknyzSQfHpt/JsmNkjywtXb5VPtTwAAAAIBdT1VdkOQVSS5K8t2F9tbae2ac415JfnJ8enFr7V1r0o8CBgAAAOx6quqc1tpd5p1jVhQwAAAAYBdUVb+WZFOSf0nyjYX21tpH5hZqDW20y6gCAADAevFTSR6R5N657hCSNj5fd+yBAQAAALugqtqS5ODW2jfnnWUWbjDvAAAAAMAO+WiSveYdYlYcQgIAAAC7pr2SfKKqzs1158BorbWj5xdp7TiEBAAAAHZBVXWPyadJ7pbk2Nba7ecUaU05hAQAAAB2Qa219yS5OskvJTkxw8k7/26emdaSQ0gAAABgF1JVP5bkoePtS0n+KcMRFveaa7A15hASAAAA2IVU1XeTvC/J8a21LWPbp1trPzLfZGvLISQAAACwa3lQksuSnFVVL6uqIzKcA2NdswcGAAAA7IKq6qZJjs5wKMm9k7wqySmttX+Za7A1ooABAAAAu7iq2jvJQ5L8amvtiHnnWQsKGAAAAED3nAMDAAAA6J4CBgAAANA9BQwAAACgewoYAAAAQPf+Px6P66tEYIAGAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distinctCounter = credit_data.nunique()   # get a series containing column name and unique value cnts \n",
    "display(distinctCounter.sort_values(ascending=True)[:25])   # top 25 features having unique values\n",
    "\n",
    "# plotting the distinct values\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "credit_data.nunique().plot.bar()\n",
    "plt.tight_layout()\n",
    "plt.title(\"Feature Unique value counts\", size=22)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "- No Null values and all the features are float dtype\n",
    "- This features are after performing PCA but are not standarized\n",
    "- Highly imbalanced dataset with fraud values ~ 0.017%\n",
    "- No feature is having a distinct values\n",
    "\n",
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data-Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = credit_data.copy().drop(\"Class\", axis=1)\n",
    "y = credit_data[\"Class\"].copy()\n",
    "\n",
    "\n",
    "## Standardization\n",
    "featuresToScale = X.drop([\"Time\"], axis=1).columns\n",
    "print(featuresToScale, len(featuresToScale))  # Class and Time removed\n",
    "\n",
    "std_scl = preprocessing.StandardScaler(copy=True)\n",
    "X.loc[:, featuresToScale] = std_scl.fit_transform(X[featuresToScale])\n",
    "print(\"Standardization completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 7))\n",
    "\n",
    "# Getting the Upper Triangle of the correlation matrix\n",
    "corr = X.corr()\n",
    "matrix = np.triu(corr)\n",
    "\n",
    "sns.heatmap(data=corr, annot=True, fmt=\".2f\", mask=matrix)\n",
    "plt.title(\"Features Correlation\", size=18, color=\"k\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "- No need to drop any features as they are generated from PCA and their is no co-linearity among each features.\n",
    "- Also, No need for feature enggineering as we don't interpret these features. Since, these are outcome of PCA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training \n",
    "\n",
    "- Before evaluating the model on test-dataset, we perform cross-validation\n",
    "- Once, statisfied with the model we will proceed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = model_selection.train_test_split(X, \n",
    "                                                                     y, \n",
    "                                                                     test_size=0.33,\n",
    "                                                                     stratify=y,\n",
    "                                                                     random_state=42,\n",
    "                                                                     shuffle=True)\n",
    "\n",
    "print(f\"TRAINING INFO: {X_train.shape} {y_train.shape}\")\n",
    "print(f\"TEST INFO: {X_test.shape} {y_test.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Model #1: Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Cross-validation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf  = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "training_scores = []\n",
    "cross_val_scores = []\n",
    "predictionsBasedOnKFolds = pd.DataFrame(data=[], index=y_train.index, columns=[0, 1])\n",
    "\n",
    "# initiate the logistic regression model\n",
    "logres = LogisticRegression(penalty=\"l2\", \n",
    "                            C=1.0,\n",
    "                            class_weight=\"balanced\",\n",
    "                            solver=\"lbfgs\",\n",
    "                            random_state=0,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "    # print(\"TRAIN:\", train_idx, \"TEST:\", val_idx)\n",
    "    \n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx, :], X_train.iloc[val_idx, :]\n",
    "    # print(\"TRAIN DATA:\", X_train_fold.shape, X_val_fold.shape)\n",
    "    \n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    # print(\"TRAIN LABEL:\", y_train_fold.shape, y_val_fold.shape)\n",
    "    \n",
    "    logres.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    y_train_preds_proba = logres.predict_proba(X_train_fold)\n",
    "    y_val_preds_proba = logres.predict_proba(X_val_fold)\n",
    "    \n",
    "    predictionsBasedOnKFolds.loc[X_val_fold.index, :] =  y_val_preds_proba\n",
    "    \n",
    "    train_log_loss = metrics.log_loss(y_train_fold, y_train_preds_proba[:, 1])\n",
    "    val_log_loss = metrics.log_loss(y_val_fold, y_val_preds_proba[:, 1])\n",
    "    print(f\"Training log_loss: {train_log_loss:.4f}\")\n",
    "    print(f\"Validation log_loss: {val_log_loss:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    training_scores.append(train_log_loss)\n",
    "    cross_val_scores.append(val_log_loss)\n",
    "\n",
    "print(\"\\nCross-validation Results...\")   \n",
    "print(f\"Average log_loss in training set: {np.mean(training_scores):.4f}\")\n",
    "print(f\"Average log_loss in validation set: {np.mean(cross_val_scores):.4f}\")\n",
    "\n",
    "print(\"\\nPredictionsBasedOnKFolds...\")\n",
    "display(predictionsBasedOnKFolds.head())\n",
    "\n",
    "loglossLogisticRegression = metrics.log_loss(y_train, predictionsBasedOnKFolds.loc[:,1])\n",
    "print('Logistic Regression Log Loss: ', loglossLogisticRegression)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Approach 2: Cross-validation -- sklearn also provides new methods for cross-validation `cross_val_score` & `cross_validate`.\n",
    "1. cross_val_score just takes on scoring metric\n",
    "2. cross_validate, we can list or tuple of scoring metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the names of the metrics\n",
    "import sklearn \n",
    "print(sklearn.metrics.get_scorer_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score_results = model_selection.cross_val_score(estimator=logres,\n",
    "                                                          X=X_train,\n",
    "                                                          y=y_train,\n",
    "                                                          scoring=\"roc_auc\",\n",
    "                                                          cv=5,\n",
    "                                                          n_jobs=-1)\n",
    "\n",
    "print(f\"AUC Score across each CV: {cross_val_score_results}\")\n",
    "print(f\"Average AUC Score : {np.mean(cross_val_score_results):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_scoring = ['roc_auc', 'precision', 'recall', 'f1', \"neg_log_loss\"]\n",
    "cross_validate_results = model_selection.cross_validate(estimator=logres,\n",
    "                                                        X=X_train,\n",
    "                                                        y=y_train,\n",
    "                                                        scoring=_scoring,\n",
    "                                                        cv=5,\n",
    "                                                        n_jobs=-1,\n",
    "                                                        verbose=0,\n",
    "                                                        pre_dispatch=2,\n",
    "                                                        return_train_score=True)\n",
    "\n",
    "print(cross_validate_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "\"Mean Training AUC\": cross_validate_results['train_roc_auc'].mean()*100,\n",
    "\"Mean Validation AUC\": cross_validate_results['test_roc_auc'].mean()*100,\n",
    "\n",
    "\"Mean Training Precision\": cross_validate_results['train_precision'].mean(),\n",
    "\"Mean Validation Precision\": cross_validate_results['test_precision'].mean(),\n",
    "\n",
    "\"Mean Training Recall\": cross_validate_results['train_recall'].mean(),\n",
    "\"Mean Validation Recall\": cross_validate_results['test_recall'].mean(),\n",
    "\n",
    "\n",
    "\"Mean Training F1 Score\": cross_validate_results['train_f1'].mean(),\n",
    "\"Mean Validation F1 Score\": cross_validate_results['test_f1'].mean(),\n",
    "\n",
    "\n",
    "\"Mean Training neg-log-loss Score\": cross_validate_results['train_neg_log_loss'].mean(),\n",
    "\"Mean Validation neg-log-loss Score\": cross_validate_results['test_neg_log_loss'].mean(),         \n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Evaluation Metrics:\n",
    "- For problems involving more balanced classes (i.e., the number of true positives is roughly similar to the number of true negatives), the confusion matrix,  AUC-Score are good.\n",
    "- But For Imbalanced dataset Precision-Recall is a better metrics. Let's explore this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_lr = pd.concat([y_train, predictionsBasedOnKFolds.loc[:, 1]], axis=1)\n",
    "preds_lr.columns = [\"true_label\", \"predicted_label\"]\n",
    "preds_lr.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_precision_recall_curve(preds_df, model_instance):\n",
    "    print(\"\\nPlotting Precision Recall Curve\")\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(preds_df[\"true_label\"], preds_df[\"predicted_label\"])\n",
    "\n",
    "    average_precision = metrics.average_precision_score(preds_df[\"true_label\"], preds_df[\"predicted_label\"])\n",
    "    print(average_precision)\n",
    "\n",
    "    plt.figure(figsize=(12, 6), tight_layout=True)\n",
    "    plt.plot(recall, precision, marker=\".\", color=\"red\", label=model_instance.__class__.__name__)\n",
    "    plt.xlabel('Recall', size=14)\n",
    "    plt.ylabel('Precision', size=14)\n",
    "    plt.legend(loc=\"upper right\", fontsize=14)\n",
    "    plt.title(f\"Precision-Recall Curve: Average Precision = {average_precision:.2f}\", fontsize=18)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "plot_precision_recall_curve(preds_df=preds_lr, model_instance=logres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "def plot_roc_auc_curve(preds_df, model_instance):\n",
    "    print(\"\\nPlotting ROC-AUC Curve\")\n",
    "    fpr, tpr, thresh = metrics.roc_curve(preds_df[\"true_label\"], preds_df[\"predicted_label\"])\n",
    "    \n",
    "    # calculate roc-auc score\n",
    "    auc_score = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6), tight_layout=True)\n",
    "    \n",
    "    # Plot the diagonal 50% line\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    \n",
    "    # Plot the FPR and TPR achieved by our model\n",
    "    plt.plot(fpr, tpr, label=f\"{model_instance.__class__.__name__ }(AUC={auc_score:.3f})\", marker=\".\")\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic: Area under the curve = {0:0.2f}'.format(auc_score), fontsize=18)\n",
    "    plt.legend(loc=\"lower right\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_roc_auc_curve(preds_lr, logres)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Model #2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "skf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=10,\n",
    "                            criterion=\"gini\",\n",
    "                            n_jobs=-1,\n",
    "                            random_state=0,\n",
    "                            max_depth=10,\n",
    "                            class_weight=\"balanced\")\n",
    "\n",
    "training_scores =  []\n",
    "cross_val_scores = []\n",
    "predictionsBasedOnKFolds = pd.DataFrame(data=[], index=y_train.index, columns=[0, 1])\n",
    "\n",
    "for train_idx, val_idx in skf.split(np.zeros(len(X_train)), y_train.ravel()):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx, :], X_train.iloc[val_idx, :]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    rf.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    train_probas = rf.predict_proba(X_train_fold)\n",
    "    val_probas = rf.predict_proba(X_val_fold)\n",
    "    \n",
    "    predictionsBasedOnKFolds.loc[X_val_fold.index, :] =  val_probas\n",
    "    \n",
    "    train_log_loss = metrics.log_loss(y_train_fold, train_probas[:, 1])\n",
    "    val_log_loss = metrics.log_loss(y_val_fold, val_probas[:, 1])\n",
    "    print(f\"Training log_loss: {train_log_loss:.4f}\")\n",
    "    print(f\"Validation log_loss: {val_log_loss:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    training_scores.append(train_log_loss)\n",
    "    cross_val_scores.append(val_log_loss)\n",
    "\n",
    "print(\"\\nCross-validation Results...\")   \n",
    "print(f\"Average log_loss in training set: {np.mean(training_scores):.4f}\")\n",
    "print(f\"Average log_loss in validation set: {np.mean(cross_val_scores):.4f}\")\n",
    "\n",
    "print(\"\\nPredictionsBasedOnKFolds...\")\n",
    "display(predictionsBasedOnKFolds.head())\n",
    "\n",
    "loglossRandomForestClassifier = metrics.log_loss(y_train, predictionsBasedOnKFolds.loc[:,1])\n",
    "print('Random ForestClassifier Log Loss: ', loglossRandomForestClassifier)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglossLogisticRegression, loglossRandomForestClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "- The training loss is quite much lesser than validation loss, the model is getting overfitted.\n",
    "- Enough it overfits the training data somewhat, the random forests has a validation log loss that is about one-tenth that of the logistic regressionsignificant improvement over the previous machine learning solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_rf = pd.concat([y_train, predictionsBasedOnKFolds.loc[:, 1]], axis=1)\n",
    "preds_rf.columns = [\"true_label\", \"predicted_label\"]\n",
    "preds_rf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see from the curve, the model can catch approximately 80% of all the fraud with approximately 80% precision. This is more impressive\n",
    "# than the approximately 80% of all the fraud the logistic regression model caught with 70% precision.\n",
    "\n",
    "plot_precision_recall_curve(preds_rf, rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "plot_roc_auc_curve(preds_df=preds_rf, model_instance=rf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "- The average precision of 0.81 of the random forests model is a clear improvement over the 0.72 average precision of the logistic regression.\n",
    "- Ther AUC Score is somewhat worse in Random forest (0.95) compared to logistic regression (0.97) But since highly imbalanced dataset so precision recall curve is better metric."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Model #3: Gradient Boosting Machine (XGBoost)\n",
    "- [Implementation of the scikit-learn API for XGBoost classification](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_xGB = {\n",
    "#     'nthread':16, #number of cores\n",
    "#     'learning rate': 0.3, #range 0 to 1, default 0.3\n",
    "#     'gamma': 0, #range 0 to infinity, default 0 \n",
    "#         # increase to reduce complexity (increase bias, reduce variance)\n",
    "#     'max_depth': 6, # range 1 to infinity, default 6\n",
    "#     'min_child_weight': 1, # range 0 to infinity, default 1\n",
    "#     'max_delta_step': 0, # range 0 to infinity, default 0\n",
    "#     'subsample': 1.0, # range 0 to 1, default 1\n",
    "#         # subsample ratio of the training examples\n",
    "#     'colsample_bytree': 1.0, #range 0 to 1, default 1 \n",
    "#         # subsample ratio of features\n",
    "#     'objective':'binary:logistic',\n",
    "#     'num_class':1,\n",
    "#     'eval_metric':'logloss',\n",
    "#     'seed':2018,\n",
    "#     'silent':1\n",
    "# }\n",
    "\n",
    "params_xGB = {\"n_estimators\": 10,\n",
    "              \"learning_rate\": 0.2,\n",
    "              \"booster\": \"gbtree\",\n",
    "              \"eval_metric\": \"logloss\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "skf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "XGBOOST = xgb.XGBClassifier(**params_xGB)\n",
    "\n",
    "training_scores =  []\n",
    "cross_val_scores = []\n",
    "predictionsBasedOnKFolds = pd.DataFrame(data=[], index=y_train.index, columns=[0, 1])\n",
    "\n",
    "for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx, :], X_train.iloc[val_idx, :]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    XGBOOST.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    train_probas = XGBOOST.predict_proba(X_train_fold)\n",
    "    val_probas = XGBOOST.predict_proba(X_val_fold)\n",
    "    \n",
    "    predictionsBasedOnKFolds.loc[X_val_fold.index, :] =  val_probas\n",
    "    \n",
    "    train_log_loss = metrics.log_loss(y_train_fold, train_probas[:, 1])\n",
    "    val_log_loss = metrics.log_loss(y_val_fold, val_probas[:, 1])\n",
    "    print(f\"Training log_loss: {train_log_loss:.4f}\")\n",
    "    print(f\"Validation log_loss: {val_log_loss:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    training_scores.append(train_log_loss)\n",
    "    cross_val_scores.append(val_log_loss)\n",
    "\n",
    "print(\"\\nPredictionsBasedOnKFolds...\")\n",
    "display(predictionsBasedOnKFolds.head())\n",
    "\n",
    "loglossXGBClassifier = metrics.log_loss(y_train, predictionsBasedOnKFolds.loc[:,1])\n",
    "print('XGBClassifier Log Loss: ', loglossXGBClassifier)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d630e0cc0c84cebedeee9ce2cc9e0c67fed820c0a3ba66cc141a6b3ba050296"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
