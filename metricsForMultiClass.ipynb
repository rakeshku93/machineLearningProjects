{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementing Evaluation  Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 1, 1, 0, 1, 1, 0, 1, 0]\n",
    "y_pred = [0, 0, 1, 0, 0, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive(y_true, y_pred):\n",
    "    tp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == yp == 1:\n",
    "            tp +=1    \n",
    "    return tp\n",
    "\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    fp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 1:\n",
    "            fp +=1    \n",
    "    return fp\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    tn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == yp == 0:\n",
    "            tn +=1    \n",
    "    return tn\n",
    "\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    fn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 0:\n",
    "            fn +=1    \n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using sklearn method: 0.55556\n",
      "Accuracy using self-defined method: 0.55556\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    counter = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == yp:\n",
    "            counter += 1\n",
    "    \n",
    "    accuracy_score = counter/len(y_true) \n",
    "    return accuracy_score\n",
    "\n",
    "print(f\"Accuracy using sklearn method: {metrics.accuracy_score(y_true, y_pred):.5f}\")\n",
    "print(f\"Accuracy using self-defined method: {accuracy(y_true, y_pred):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision using sklearn method: 0.66667\n",
      "Precision using self-defined method: 0.66667\n",
      "\n",
      "Recall using sklearn method: 0.40000\n",
      "Recall using self-defined method: 0.40000\n",
      "\n",
      "F1-score using sklearn method: 0.50000\n",
      "F1-score using self-defined method: 0.50000\n"
     ]
    }
   ],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    return tp/(tp+fp)\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    \n",
    "    if (p+r) != 0:\n",
    "        return 2*p*r/(p+r)\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "print(f\"Precision using sklearn method: {metrics.precision_score(y_true, y_pred):.5f}\")\n",
    "print(f\"Precision using self-defined method: {precision(y_true, y_pred):.5f}\")\n",
    "\n",
    "print(f\"\\nRecall using sklearn method: {metrics.recall_score(y_true, y_pred):.5f}\")\n",
    "print(f\"Recall using self-defined method: {recall(y_true, y_pred):.5f}\")\n",
    "\n",
    "print(f\"\\nF1-score using sklearn method: {metrics.f1_score(y_true, y_pred):.5f}\")\n",
    "print(f\"F1-score using self-defined method: {f1_score(y_true, y_pred):.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation of Precision for Multi-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n",
    "y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Macro averaged Precision: calculate Precision for all classes individually and then average them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_recall(y_true, y_pred):\n",
    "    \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    precision = 0\n",
    "    \n",
    "    for class_ in range(num_classes):\n",
    "        tempClass = [1 if label == class_ else 0 for label in y_true]\n",
    "        tempPred =  [1 if label == class_ else 0 for label in y_pred]\n",
    "        \n",
    "        tp = true_positive(tempClass, tempPred)\n",
    "        fp = false_positive(tempClass, tempPred)\n",
    "        \n",
    "        temp_precision = tp/(tp+fp)\n",
    "        \n",
    "        precision += temp_precision\n",
    "    \n",
    "    precision /= num_classes    \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Micro averaged Precison:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_precison(y_true, y_pred):\n",
    "    \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    precision = 0\n",
    "    \n",
    "    for class_ in range(num_classes):\n",
    "        tempClass = [1 if label == class_ else 0 for label in y_true]\n",
    "        tempPred =  [1 if label == class_ else 0 for label in y_pred]\n",
    "        \n",
    "        tp += true_positive(tempClass, tempPred)\n",
    "        fp += false_positive(tempClass, tempPred)\n",
    "            \n",
    "    precision = tp/(tp+fp)    \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Weighted averaged Precison:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Precison using sklearn method: 0.3981\n",
      "Weighted Precision using self-defined method: 0.3981\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def weighted_precison(y_true, y_pred):\n",
    "    \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    class_counter = Counter(y_true)\n",
    "    weighted_precison = 0\n",
    "    for class_ in range(num_classes):\n",
    "        tempClass = [1 if label == class_ else 0 for label in y_true]\n",
    "        tempPred =  [1 if label == class_ else 0 for label in y_pred]\n",
    "        \n",
    "        tp = true_positive(tempClass, tempPred)\n",
    "        fp = false_positive(tempClass, tempPred)\n",
    "           \n",
    "        temp_precision = tp/(tp+fp)\n",
    "        \n",
    "        weighted_precison += class_counter[class_] * temp_precision   \n",
    "            \n",
    "    precision = weighted_precison/len(y_true)   \n",
    "    return precision\n",
    "\n",
    "print(f\"Weighted Precison using sklearn method: {metrics.precision_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "print(f\"Weighted Precision using self-defined method: {weighted_precison(y_true, y_pred):.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation of Recall for Multi-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n",
    "y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Macro averaged recall: calculate recall for all classes individually and then average them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_recall(y_true, y_pred):\n",
    "    \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    recall = 0\n",
    "    \n",
    "    for class_ in range(num_classes):\n",
    "        tempClass = [1 if label == class_ else 0 for label in y_true]\n",
    "        tempPred =  [1 if label == class_ else 0 for label in y_pred]\n",
    "        \n",
    "        tp = true_positive(tempClass, tempPred)\n",
    "        fn = false_negative(tempClass, tempPred)\n",
    "        \n",
    "        temp_recall = tp/(tp+fn)\n",
    "        \n",
    "        recall += temp_recall\n",
    "    \n",
    "    recall /= num_classes    \n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Recall using sklearn method: 0.41667\n",
      "Macro Recall using self-defined method: 0.41667\n"
     ]
    }
   ],
   "source": [
    "print(f\"Macro Recall using sklearn method: {metrics.recall_score(y_true, y_pred, average='macro'):.5f}\")\n",
    "print(f\"Macro Recall using self-defined method: {macro_recall(y_true, y_pred):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_recall(y_true, y_pred):\n",
    "    \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    recall = 0\n",
    "    \n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    for class_ in range(num_classes):\n",
    "        tempClass = [1 if label == class_ else 0 for label in y_true]\n",
    "        tempPred =  [1 if label == class_ else 0 for label in y_pred]\n",
    "        \n",
    "        tp += true_positive(tempClass, tempPred)\n",
    "        fn += false_negative(tempClass, tempPred)\n",
    "        \n",
    "    recall = tp/(tp+fn)    \n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Recall using sklearn method: 0.44444\n",
      "Micro Recall using self-defined method: 0.44444\n"
     ]
    }
   ],
   "source": [
    "print(f\"Micro Recall using sklearn method: {metrics.recall_score(y_true, y_pred, average='micro'):.5f}\")\n",
    "print(f\"Micro Recall using self-defined method: {micro_recall(y_true, y_pred):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def weighted_recall(y_true, y_pred):\n",
    "    \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    class_value_counts = Counter(y_true)\n",
    "    recall = 0\n",
    "    \n",
    "    for class_ in range(num_classes):\n",
    "        tempClass = [1 if label == class_ else 0 for label in y_true]\n",
    "        tempPred =  [1 if label == class_ else 0 for label in y_pred]\n",
    "        \n",
    "        tp = true_positive(tempClass, tempPred)\n",
    "        fn = false_negative(tempClass, tempPred)\n",
    "        \n",
    "        temp_recall = tp/(tp+fn)\n",
    "        \n",
    "        weighted_recall = class_value_counts[class_] * temp_recall\n",
    "        \n",
    "        recall += weighted_recall\n",
    "    \n",
    "    overall_recall  = recall / len(y_true)   \n",
    "    return overall_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Recall using sklearn method: 0.4444\n",
      "Weighted Recall using self-defined method: 0.4444\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weighted Recall using sklearn method: {metrics.recall_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "print(f\"Weighted Recall using self-defined method: {weighted_recall(y_true, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation of f1-score for Multi-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted f1-score using sklearn method: 0.4127\n",
      "Weighted f1-score using self-defined method: 0.4127\n"
     ]
    }
   ],
   "source": [
    "def weighted_f1_score(y_true, y_pred):\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    class_value_counts = Counter(y_true)\n",
    "    f1_score = 0\n",
    "    \n",
    "    for class_ in range(num_classes):\n",
    "        temp_class = [1 if label == class_ else 0 for label in y_true]\n",
    "        temp_pred = [1 if label == class_ else 0 for label in y_pred]\n",
    "        \n",
    "        p = precision(temp_class, temp_pred)\n",
    "        r = recall(temp_class, temp_pred)\n",
    "        \n",
    "        # calculate f1_score for each class\n",
    "        if (p+r) != 0:\n",
    "            temp_f1 = 2*p*r/(p+r)\n",
    "        else:\n",
    "            temp_f1 = 0\n",
    "         \n",
    "        # calculate weighted_f1_score for each class \n",
    "        # by multiplying class sample counts and individual class f1 score           \n",
    "        weighted_f1 = class_value_counts[class_] * temp_f1\n",
    "        \n",
    "        f1_score += weighted_f1\n",
    "        \n",
    "    overall_f1_score = f1_score / len(y_true)\n",
    "    return overall_f1_score\n",
    "\n",
    "print(f\"Weighted f1-score using sklearn method: {metrics.f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "print(f\"Weighted f1-score using self-defined method: {weighted_f1_score(y_true, y_pred):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56b3ff06c847d0c4f7be3763dc97c6491a535e13ce8210240ca7cce9ff5cd5ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
